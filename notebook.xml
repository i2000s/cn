<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xiaodong Qi's academic NoteBlog</title>
 <link href="/exploringquantumworld.xml" rel="self"/>
 <link href=""/>
 <updated>2016-02-15T14:51:26+00:00</updated>
 <id></id>
 <author>
   <name>Xiaodong Qi</name>
   <email>i2000s@hotmail.com</email>
 </author>

 
 <entry>
   <title>Dispersion relations for linear systems of PDEs</title>
   <link href="/2014/05/28/dispersion_relations.html"/>
   <updated>2014-05-28T00:00:00+00:00</updated>
   <id>h/2014/05/28/dispersion_relations</id>
   <content type="html">&lt;p&gt;Fourier analysis is an essential tool for understanding the behavior of
solutions to linear equations.  Often, this analysis is introduced to students in the
context of scalar equations with real coefficients.  If nothing more is said,
students may mistakenly apply assumptions based on the scalar case to systems,
leading to erroneous conclusions.  I&amp;#39;m surprised at how often I&amp;#39;ve seen
this, and I&amp;#39;ve even made the mistake myself.&lt;/p&gt;

&lt;h2&gt;Scalar equations&lt;/h2&gt;

&lt;p&gt;Students in any undergraduate PDE course learn that solutions of the heat equation&lt;/p&gt;

&lt;p&gt;$$
\label{heat}
u&lt;em&gt;t(x,t) = u&lt;/em&gt;{xx}(x,t)
$$&lt;/p&gt;

&lt;p&gt;diffuse in time whereas solutions of the wave equation&lt;/p&gt;

&lt;p&gt;$$
\label{wave}
u&lt;em&gt;{tt} = u&lt;/em&gt;{xx}
$$&lt;/p&gt;

&lt;p&gt;oscillate in time without growing or decaying.  They may even be introduced to
a general approach for the Cauchy problem: given an evolution equation&lt;/p&gt;

&lt;p&gt;$$ \label{evol}
u&lt;em&gt;t = \sum&lt;/em&gt;{j=0}^n a_j \frac{\partial^j u}{\partial x^j},
$$&lt;/p&gt;

&lt;p&gt;one inserts the Fourier mode solution&lt;/p&gt;

&lt;p&gt;$$ \label{fourier}
u(x,t) = e^{i(kx - \omega(k) t)}
$$&lt;/p&gt;

&lt;p&gt;to obtain&lt;/p&gt;

&lt;p&gt;$$-i\omega(k) = \sum&lt;em&gt;{j=0}^n a&lt;/em&gt;j (ik)^j$$&lt;/p&gt;

&lt;p&gt;or simply&lt;/p&gt;

&lt;p&gt;$$\omega(k) = \sum&lt;em&gt;{j=0}^n a&lt;/em&gt;j i^{j+1} k^j.$$&lt;/p&gt;

&lt;p&gt;The function $\omega(k)$ is often referred to as the &lt;em&gt;dispersion relation&lt;/em&gt; for the PDE.  Any solution can be expressed as a sum of Fourier modes, and each mode propagates in a manner dictated by the dispersion relation.  It&amp;#39;s easy to see that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If $\omega(k)$ is &lt;strong&gt;real&lt;/strong&gt;, then energy is conserved and each mode simply translates.  This occurs if only odd-numbered spatial derivatives appear in the evolution equation  \eqref{evol}.&lt;/li&gt;
&lt;li&gt;If $\omega(k)$ has &lt;strong&gt;negative imaginary part&lt;/strong&gt;, energy decays in time.  The heat equation \eqref{heat} behaves this way.&lt;/li&gt;
&lt;li&gt;If $\omega(k)$ has &lt;strong&gt;positive imaginary part&lt;/strong&gt;, then the energy will grow exponentially in time.  This doesn&amp;#39;t usually occur in physical systems.  An example of this behavior is obtained by changing the sign of the right side in the heat equation to get $u&lt;em&gt;t = - u&lt;/em&gt;{xx}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What about the wave equation, which has two time derivatives?  Using the same Fourier mode ansatz
\eqref{fourier}, one obtains
$$
\begin{align}
\omega^2 &amp;amp; = k^2
\end{align}
$$
or $\omega = \pm k$.  Since $\omega$ is real, energy is conserved.&lt;/p&gt;

&lt;p&gt;In the discussion above, we have assumed that $u$ is a scalar and that the coefficients $a_j$ are real.  Many undergraduate courses stop at this point, and students are left with the intuition that &lt;strong&gt;even-numbered derivative terms are diffusive&lt;/strong&gt; while &lt;strong&gt;odd-numbered derivative terms are dispersive&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In practice, we often deal with systems of PDEs or PDEs with complex coefficients, and this intuition is then no longer correct.  There is nothing deep or mysterious about this topic, but it&amp;#39;s easy to jump to incorrect conclusions if one is not careful.  To take a common example, consider the time-dependent Schroedinger equation:
$$i \psi&lt;em&gt;t = \psi&lt;/em&gt;{xx} + V\psi.$$
At first glance, we have on the right side a diffusion term ($\psi_{xx}$) and a reaction term ($V\psi$).  But what about that pesky factor of $i$ (the imaginary unit) on the left hand side?  It&amp;#39;s easy to find the answer using the usual ansatz, but let&amp;#39;s take a little detour first.&lt;/p&gt;

&lt;h2&gt;Systems of equations&lt;/h2&gt;

&lt;p&gt;Consider the linear system
$$
\begin{align&lt;em&gt;}
u_t = A  \frac{\partial^j u}{\partial x^j},
\end{align&lt;/em&gt;}
$$
where $u\in \mathbb{R}^m$ and $A$ is a square real matrix.
Let $\lambda&lt;em&gt;m$ and $s&lt;/em&gt;m$ denote the eigenvalues and eigenvectors (respectively) of $A$.
Inserting the Fourier mode solution
$$u(x,t) = s&lt;em&gt;m e^{i(kx - \omega(k) t)},$$
we obtain
$$\omega(k) = i^{j+1} k^j \lambda&lt;/em&gt;m s&lt;em&gt;m,$$
and any solution can be written as a superposition of these.  We see now that the behavior of the energy with respect to time depends on both the number $j$ of spatial derivatives and the nature of the eigenvalues of $A$.  For instance, if $j=1$ and $A$ has imaginary eigenvalues, energy is conserved.  We can obtain just such an example by rewriting the wave equation \eqref{wave} as a first-order system:
$$
\begin{align}
u&lt;/em&gt;t &amp;amp; = v&lt;em&gt;x \label{w1} \
v&lt;/em&gt;t &amp;amp; = u_x. \label{w2}
\end{align}
$$
(If you&amp;#39;re not familiar with this, just differentiate \eqref{w1} w.r.t. $t$ and \eqref{w2} w.r.t. $x$, then equate partial derivatives to get back the second-order wave equation \eqref{wave}).  We have a linear system with $j=1$ and 
$$ A = \begin{pmatrix}
0 &amp;amp; 1 \ 1 &amp;amp; 0
\end{pmatrix}.$$
This matrix has eigenvalues $\lambda=\pm 1$, so $\omega(k)$ has zero imaginary part.&lt;/p&gt;

&lt;p&gt;In this example, our intuition from the scalar case works: our first-order system, with only odd-numbered derivatives, leads to wave-like behavior.  But notice that if $A$ had imaginary eigenvalues, our intuition would be wrong; for instance, the system
$$
\begin{align&lt;em&gt;}
u&lt;em&gt;t &amp;amp; = -v&lt;/em&gt;x \
v&lt;em&gt;t &amp;amp; = u&lt;/em&gt;x,
\end{align&lt;/em&gt;}
$$
corresponding to the second-order equation $u&lt;em&gt;{tt} = - u&lt;/em&gt;{xx},$ admits exponentially growing solutions.&lt;/p&gt;

&lt;h2&gt;Scalar problems with complex coefficients&lt;/h2&gt;

&lt;p&gt;Now that we understand the dispersion relation for systems, it&amp;#39;s easy to understand the dispersion relation for the Schrodinger equation.  Multiply by $-i$ to get
$$\psi&lt;em&gt;t = -i\psi&lt;/em&gt;{xx} + -iV\psi.$$
Now we can think of this in the same way as a system, where the coefficient matrices have purely imaginary eigenvalues.  Then it&amp;#39;s clear that the (even-derivative) terms on the right hand side are both related to wave behavior (i.e., energy is conserved).&lt;/p&gt;

&lt;h2&gt;Systems with derivatives of different orders&lt;/h2&gt;

&lt;p&gt;In the most general case, we have systems of linear PDEs with multiple spatial derivatives
of different order:
$$ \label{gensys}
u&lt;em&gt;t = \sum&lt;/em&gt;{j=0}^n A_j  \frac{\partial^j u}{\partial x^j}.
$$&lt;/p&gt;

&lt;p&gt;Here&amp;#39;s a real example from my research.  It comes from homogenization of
the wave equation in a spatially varying medium 
(see Equation (5.17) of &lt;a href=&quot;http://faculty.washington.edu/rjl/pubs/solitary/40815.pdf&quot;&gt;this paper&lt;/a&gt; for 
more details).  It&amp;#39;s the wave equation plus some second-derivative terms:
$$
u&lt;em&gt;t = v&lt;/em&gt;x + v&lt;em&gt;{xx} \
v&lt;/em&gt;t = u&lt;em&gt;x - u&lt;/em&gt;{xx}.
$$
You might (if you hadn&amp;#39;t read the example above) assume that this system
is dissipative due to the second derivatives.
This system is of the form \eqref{gensys} with
$$
\begin{align}
A&lt;em&gt;1 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \ 1 &amp;amp; 0
\end{pmatrix}
&amp;amp;
A&lt;/em&gt;2 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \ -1 &amp;amp; 0
\end{pmatrix}.
\end{align}
$$
Of course, $A&lt;em&gt;1$ has real eigenvalues and leads to wave-like behavior.
But $A&lt;/em&gt;2$ has pure imaginary eigenvalues, so it also leads to wave-like
behavior! The second derivative terms are &lt;em&gt;dispersive&lt;/em&gt;.  In fact, it&amp;#39;s
easy to show that the energy $E=u^2+v^2$ is a conserved quantity for this
system (try it!).&lt;/p&gt;

&lt;p&gt;Strictly speaking, Fourier analysis like what we&amp;#39;ve described can&amp;#39;t usually be
applied to \eqref{gensys} because the matrices $A_j$ will not generally be simultaneously
diagonalizable (though this analysis can still give us intuition for what
each set of terms may do).  Worse yet, the individual matrices may not
be diagonalizable.  Let&amp;#39;s illustrate with a simple case.&lt;/p&gt;

&lt;p&gt;Returning to the wave equation, let&amp;#39;s consider a different way of writing it as a system:
$$
\begin{align&lt;em&gt;}
u&lt;em&gt;t &amp;amp; = v \ 
v&lt;/em&gt;t &amp;amp; = u_{xx}.
\end{align&lt;/em&gt;}
$$
It&amp;#39;s easy to check that this system is equivalent to the wave equation -- but notice that it&amp;#39;s composed of parts with only even derivatives! (&lt;em&gt;reaction&lt;/em&gt; and &lt;em&gt;diffusion&lt;/em&gt; equations in the terminology of scalar PDEs).  This system is of the form \eqref{gensys} with
$$
\begin{align}
A&lt;em&gt;0 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \ 0 &amp;amp; 0
\end{pmatrix}
&amp;amp;
A&lt;/em&gt;2 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 0 \ 1 &amp;amp; 0
\end{pmatrix}.
\end{align}
$$
Notice that both eigenvalues of both matrices are equal to zero.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes 2014.03.03</title>
   <link href="/2014/03/03/notes.html"/>
   <updated>2014-03-03T00:00:00+00:00</updated>
   <id>h/2014/03/03/notes</id>
   <content type="html">&lt;p&gt;Finally figured out what was wrong with the stability regions for the deferred correction methods in Nodepy when $\theta \ne 0$.
See &lt;a href=&quot;https://bitbucket.org/ketch/rkextrapolation/src/cce934e20cf514c8c5450e7ad09f5774052ff575/code/SDC%20Stability%20regions%20when%20theta%20is%20nonzero.ipynb?at=master&quot;&gt;these&lt;/a&gt;
&lt;a href=&quot;https://bitbucket.org/ketch/rkextrapolation/src/cce934e20cf514c8c5450e7ad09f5774052ff575/code/Reproduce_DC_stability_region.ipynb?at=master&quot;&gt;notebooks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also sat with Roland and got the latest version of PeanoClaw running on my workstation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes 2014.02.27</title>
   <link href="/2014/02/27/notes.html"/>
   <updated>2014-02-27T00:00:00+00:00</updated>
   <id>h/2014/02/27/notes</id>
   <content type="html">&lt;p&gt;Investigated stability regions for high order deferred correction schemes; see &lt;a href=&quot;https://bitbucket.org/ketch/rkextrapolation/src/a78b4aa2d336491cac35c1df81e703ce103d6937/SDC%20Stability%20regions%20when%20theta%20is%20nonzero.ipynb&quot;&gt;this notebook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, after about a year of searching, found a way to redirect all output from distutils
to a file.  This will avoid the massive amount of warnings that are currently printed to the
screen when installing PyClaw.  See the patch &lt;a href=&quot;https://github.com/clawpack/clawpack/pull/35&quot;&gt;here&lt;/a&gt;,
based on &lt;a href=&quot;http://stackoverflow.com/a/11632982/786902&quot;&gt;this StackOverflow answer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also put together an &lt;a href=&quot;http://nbviewer.ipython.org/urls/dl.dropboxusercontent.com/u/656693/shallow_water_diffraction.ipynb&quot;&gt;IPython notebook on shallow water solitary waves over periodic
bathymetry&lt;/a&gt;.
It will be in the Github repo soon.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The Schrodinger equation is not a reaction-diffusion equation</title>
   <link href="/2014/02/22/schrodinger-is-not-diffusion.html"/>
   <updated>2014-02-22T00:00:00+00:00</updated>
   <id>h/2014/02/22/schrodinger-is-not-diffusion</id>
   <content type="html">&lt;p&gt;Recently, a stackexchange answer claimed that &lt;a href=&quot;http://scicomp.stackexchange.com/a/10878/123&quot;&gt;the Schrodinger equation is effectively a reaction-diffusion equation&lt;/a&gt;.  I&amp;#39;ll set aside semantic arguments about the meaning of &amp;quot;effectively&amp;quot;, and give a more obvious example to explain why I think this statement is misleading.&lt;/p&gt;

&lt;p&gt;Consider the wave equation&lt;/p&gt;

&lt;p&gt;$$u&lt;em&gt;{tt} = u&lt;/em&gt;{xx}$$&lt;/p&gt;

&lt;p&gt;Introducing a new variable $v=u_t$ we can rewrite the wave equation as&lt;/p&gt;

&lt;p&gt;$$
\begin{align&lt;em&gt;}
v&lt;em&gt;t &amp;amp; = u&lt;/em&gt;{xx} \
u_t &amp;amp; = v.
\end{align&lt;/em&gt;}
$$&lt;/p&gt;

&lt;p&gt;Observe that the first of these equation is the diffusion equation, while the second is a reaction equation.  Thus we have reaction-diffusion!  &lt;/p&gt;

&lt;p&gt;Right?&lt;/p&gt;

&lt;p&gt;Wrong.  We&amp;#39;ve disguised the true nature of this equation by applying our intuition (which is based on scalar PDEs) to a system of PDEs.  In the same way, the &amp;quot;reaction-diffusion&amp;quot; label for Schrodinger is obtained by applying intuition based on PDEs with real coefficients to a PDE with complex coefficients.&lt;/p&gt;

&lt;p&gt;Of course, in both cases you can use numerical methods that are appropriate for
reaction-diffusion problems in order to solve a wave equation.&lt;br&gt;
&lt;a href=&quot;http://nbviewer.ipython.org/github/ketch/exposition/blob/master/Wave%20equation%20as%20reaction-diffusion.ipynb&quot;&gt;Here is a quick ipython notebook implementation of the obvious method for the system above&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes 2014.02.22</title>
   <link href="/2014/02/22/notes.html"/>
   <updated>2014-02-22T00:00:00+00:00</updated>
   <id>h/2014/02/22/notes</id>
   <content type="html">&lt;p&gt;Discussed time stepping for aeroacoustics with Antony Jameson at Stanford.  Also reviewed a couple of his group&amp;#39;s papers on high order flux reconstruction schemes.&lt;/p&gt;

&lt;h3&gt;Insights from von Neumann analysis of high-order flux reconstruction schemes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Vincent, Castonguay, Jameson&lt;/li&gt;
&lt;li&gt;JCP 2011&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Investigate a 1-parameter family of stable flux reconstruction methods suggested by earlier work.  For certain parameter values you get DG or SD schemes.  Some values admit spurious modes.  The size of the largest stable step size and the order of accuracy are determined as a function of the parameter (c).  Nonlinear 2D experimental results are predicted relatively well by the 1D von Neumann analysis.&lt;/p&gt;

&lt;p&gt;Section 4 is a nice description of how to do von Neumann analysis for FE methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3&gt;On the Non-linear Stability of Flux Reconstruction Schemes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Jameson, Vincent, Castonguay&lt;/li&gt;
&lt;li&gt;J. Sci. Comput. 2011&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;They look at energy stability in a very general way.  Nonlinear stability depends on solution point locations, and on the accuracy of the determination of the transformed flux.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Time integration ideas that could be useful for aeronautics simulations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Optimization of stability regions&lt;/li&gt;
&lt;li&gt;Multirate time stepping.  Some work has been done in &lt;a href=&quot;http://dx.doi.org/10.1016/j.jcp.2010.05.028&quot;&gt;Nonuniform time-step Runge–Kutta discontinuous Galerkin method for Computational Aeroacoustics&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Large time step methods.  See &lt;a href=&quot;http://dx.doi.org/10.1016/j.jcp.2011.06.008&quot;&gt;A class of large time step Godunov schemes for hyperbolic conservation laws and applications&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Large time step methods might work very well as the coarse propagator for parareal-type algorithms.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The parallel EPPEER code</title>
   <link href="/2012/10/17/eppeer.html"/>
   <updated>2012-10-17T00:00:00+00:00</updated>
   <id>h/2012/10/17/eppeer</id>
   <content type="html">&lt;p&gt;I tried out the EPPEER code, which uses two-step Runge-Kutta methods and
OpenMP, because I’m thinking of writing a shared-memory parallel ODE
solver code myself.&lt;/p&gt;

&lt;p&gt;I downloaded the code from&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.mathematik.uni-marburg.de/%7Eschmitt/peer/eppeer.zip&quot; title=&quot;Go to wiki page&quot;&gt;http://www.mathematik.uni-marburg.de/~schmitt/peer/eppeer.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;unzipped, and ran&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;gfortran -c mbod4h.f90 
gfortran -c ivprkp.f90 
gfortran -c -fopenmp ivpepp.f90 
gfortran -fopenmp ivprkp.o ivpepp.o mbod4h.o ivp_pmain.f90
./a.out
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I had to fix one line that was trying to open a logfile and failed. I
also set&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;export OMP_NUM_THREADS=4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This runs the code with increasingly tight tolerances on a 400-body
problem. The output was (I killed it before it finished the really tight
tolerance run(s)&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; tol, err, otime, cpu  0.10E-01 0.10702      2.9556      10.534    
 steps,rej,nfcn:  337   88     1399
 tol, err, otime, cpu  0.10E-02 0.93692E-01  4.9853      18.585    
 steps,rej,nfcn:  605  159     2465
 tol, err, otime, cpu  0.10E-03 0.66604E-01  7.9798      30.365    
 steps,rej,nfcn:  994  244     4015
 tol, err, otime, cpu  0.10E-04 0.47637E-01  12.026      46.477    
 steps,rej,nfcn: 1534  324     6175
 tol, err, otime, cpu  0.10E-05 0.24241E-01  18.239      70.756    
 steps,rej,nfcn: 2338  415     9391
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If I understand correctly, the last column is total CPU time; the next
to last is wall time. For comparison, I ran it without parallelism:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;export OMP_NUM_THREADS=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then I got the following:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; tol, err, otime, cpu  0.10E-01 0.10702      10.382      10.382    
 steps,rej,nfcn:  337   88     1399
 tol, err, otime, cpu  0.10E-02 0.93692E-01  18.297      18.297    
 steps,rej,nfcn:  605  159     2465
 tol, err, otime, cpu  0.10E-03 0.66604E-01  29.814      29.815    
 steps,rej,nfcn:  994  244     4015
 tol, err, otime, cpu  0.10E-04 0.47637E-01  45.854      45.855    
 steps,rej,nfcn: 1534  324     6175
 tol, err, otime, cpu  0.10E-05 0.24241E-01  69.725      69.726    
 steps,rej,nfcn: 2338  415     9391
 tol, err, otime, cpu  0.10E-06 0.53727E-02  105.47      105.48    
 steps,rej,nfcn: 3539  484    14195
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The numbers of function evaluations were identical, confirming that the
computations being performed were the same. The speedup (about 3x) is
very nice. We should be able to achieve something similar with
extrapolation.&lt;/p&gt;

&lt;p&gt;These results are actually plotted in &lt;a href=&quot;http://www.mathematik.uni-marburg.de/%7Eschmitt/peer/man_epp.pdf&quot;&gt;the user guide&lt;/a&gt;, at the end of
Section 4.&lt;/p&gt;

&lt;p&gt;This was originally posted on &lt;a href=&quot;https://mathwiki.kaust.edu.sa/david/eppeer&quot;&gt;mathwiki&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Blogging an iPython notebook with Jekyll</title>
   <link href="/2012/10/11/blogging_ipython_notebooks_with_jekyll.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/blogging_ipython_notebooks_with_jekyll</id>
   <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Update as of December 2014: Don&amp;#39;t bother using what&amp;#39;s below; go to 
&lt;a href=&quot;http://cscorley.github.io/2014/02/21/blogging-with-ipython-and-jekyll/&quot;&gt;Christop Corley&amp;#39;s blog&lt;/a&gt;
for a much better setup!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;#39;ve been playing around with &lt;a href=&quot;http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html&quot;&gt;iPython notebooks&lt;/a&gt;
for a while and planning to use them instead of &lt;a href=&quot;http://www.sagemath.org/&quot;&gt;SAGE&lt;/a&gt; 
worksheets for my numerical analysis course next spring.  As a warmup,
I wrote an iPython notebook explaining a bit about internal stability of Runge-Kutta 
methods and showing some new research results using &lt;a href=&quot;http://numerics.kaust.edu.sa/nodepy/&quot;&gt;NodePy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also wanted to post the notebook on my blog here; the ability to more easily
include math and code in blog posts was one of my main motivations for moving
away from Blogger to my own site.  I first tried following &lt;a href=&quot;http://blog.fperez.org/2012/09/blogging-with-ipython-notebook.html&quot;&gt;the instructions given
by Fernando Perez&lt;/a&gt;.
That was quite painless and worked flawlessly, using &lt;code&gt;nbconvert.py&lt;/code&gt; to convert the
.ipynb file directly to HTML, with graphics embedded.  The only issue was that I didn&amp;#39;t love
the look of the output quite as much as I love how Carl Boettiger&amp;#39;s Markdown + Jekyll
posts with code and math look (see an example &lt;a href=&quot;http://www.carlboettiger.info/2012/09/14/analytic-solution-to-multiple-uncertainty.html&quot;&gt;here&lt;/a&gt;).  Besides, Markdown is so much nicer
than HTML, and &lt;code&gt;nbconvert.py&lt;/code&gt; has a Markdown output option.&lt;/p&gt;

&lt;p&gt;So I tried the markdown option:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;nbconvert.py my_nb.ipynb -f markdown
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I copied the result to my &lt;code&gt;_posts/&lt;/code&gt; directory, added the &lt;a href=&quot;https://github.com/mojombo/jekyll/wiki/YAML-Front-Matter&quot;&gt;YAML front-matter&lt;/a&gt; that
Jekyll expects, and took a look.  Everything
was great except that all my plots were gone, of course.  After considering a
few options, I decided for now to put plots for such posts in a subfolder 
&lt;code&gt;jekyll_images/&lt;/code&gt; of my public Dropbox folder.  Then it was a simple matter
of search/replace all the paths to the images.  At that point, it looked
great; you can see the &lt;a href=&quot;https://github.com/ketch/nodepy/blob/master/examples/Internal_stability.ipynb&quot;&gt;source&lt;/a&gt;
and the &lt;a href=&quot;http://davidketcheson.info/2012/10/11/Internal_stability.html&quot;&gt;result&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The only issue was that I didn&amp;#39;t want to manually do all that work every time.
I considered creating a new Converter class in &lt;code&gt;nbconvert&lt;/code&gt; to handle it,
but finally decided that it would be more convenient to just write a shell
script that calls &lt;code&gt;nbconvert&lt;/code&gt; and then operates on the result.&lt;br&gt;
Here  it is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#!/bin/bash

fname=$1

nbconvert.py ${fname}.ipynb -f markdown
sed  -i &amp;#39;&amp;#39; &amp;quot;s#${fname}_files#https:\/\/dl.dropbox.com\/u\/656693\/jekyll_images\/${fname}_files#g&amp;quot;  ${fname}.md

dt=$(date &amp;quot;+%Y-%m-%d&amp;quot;)

echo &amp;quot;0a
---
layout:    post
time:      ${dt}
title:     TITLE-ME
subtitle:  SUBTITLE-ME
tags:      TAG-ME
---
.
w&amp;quot; | ed ${fname}.md

mv ${fname}.md ~/labnotebook/_posts/${dt}-${fname}.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It&amp;#39;s also on Github &lt;a href=&quot;https://github.com/ketch/labnotebook/blob/master/nbconv.sh&quot;&gt;here&lt;/a&gt;.
This was a nice educational exercise in constructing shell scripts, in which I learned or re-learned:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;how to use command-line arguments&lt;/li&gt;
&lt;li&gt;how to use sed and ed&lt;/li&gt;
&lt;li&gt;how to use data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can expect a lot more iPython-notebook based posts in the future.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Internal stability of Runge-Kutta methods</title>
   <link href="/2012/10/11/Internal_stability.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/Internal_stability</id>
   <content type="html">&lt;p&gt;Note: this post was generated from an iPython notebook.  You can &lt;a href=&quot;https://github.com/ketch/nodepy/blob/master/examples/Internal_stability.ipynb&quot;&gt;download the
notebook from github&lt;/a&gt; and execute all the code yourself.&lt;/p&gt;

&lt;p&gt;Internal stability deals with the growth of errors (such as roundoff) introduced at the Runge-Kutta stages during a single Runge-Kutta step.  It is usually important only for methods with a large number of stages, since that is when the internal amplification factors can be large.  An excellent explanation of internal stability is given in &lt;a href=&quot;http://oai.cwi.nl/oai/asset/1652/1652A.pdf&quot;&gt;this paper&lt;/a&gt;.  Here we demonstrate some tools for studying internal stability in NodePy.&lt;/p&gt;

&lt;p&gt;First, let&amp;#39;s load a couple of RK methods:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nodepy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadRKM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;RK44&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadRKM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;SSP104&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Classical RK4
The original four-stage, fourth-order method of Kutta
 0   |
 1/2 |  1/2
 1/2 |  0    1/2
 1   |  0    0    1
_____|____________________
     |  1/6  1/3  1/3  1/6
SSPRK(10,4)
The optimal ten-stage, fourth order SSP Runge-Kutta method
 0   |
 1/6 |  1/6
 1/3 |  1/6   1/6
 1/2 |  1/6   1/6   1/6
 2/3 |  1/6   1/6   1/6   1/6
 1/3 |  1/15  1/15  1/15  1/15  1/15
 1/2 |  1/15  1/15  1/15  1/15  1/15  1/6
 2/3 |  1/15  1/15  1/15  1/15  1/15  1/6   1/6
 5/6 |  1/15  1/15  1/15  1/15  1/15  1/6   1/6   1/6
 1   |  1/15  1/15  1/15  1/15  1/15  1/6   1/6   1/6   1/6
_____|____________________________________________________________
     |  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Absolute stability regions&lt;/h2&gt;

&lt;p&gt;First we can use NodePy to plot the region of absolute stability for each method.  The absolute stability region is the set&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$\{ z \in C : |\phi (z)|\le 1 \}$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;where $\phi(z)$ is the &lt;em&gt;stability function&lt;/em&gt; of the method:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$1 + z b^T (I-zA)^{-1}$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;If we solve $u&amp;#39;(t) = \lambda u$ with a given method, then $z=\lambda \Delta t$ must lie inside this region or the computation will be unstable.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stability_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_stability_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;         4          3       2
0.04167 x + 0.1667 x + 0.5 x + 1 x + 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_00.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stability_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_stability_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;           10             9            8             7           6
3.969e-09 x  + 2.381e-07 x + 6.43e-06 x + 0.0001029 x + 0.00108 x
            5           4          3       2
 + 0.00787 x + 0.04167 x + 0.1667 x + 0.5 x + 1 x + 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;h1&gt;Internal stability&lt;/h1&gt;

&lt;p&gt;The stability function tells us by how much errors from one step are amplified in the next one.  This is important since we introduce truncation errors at every step.  However, we also introduce roundoff errors at the each stage within a step.  Internal stability tells us about the growth of those.  Internal stability is typically less important than (step-by-step) absolute stability for two reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Roundoff errors are typically much smaller than truncation errors, so moderate amplification of them typically is not significant&lt;/li&gt;
&lt;li&gt;Although the propagation of stage errors within a step is governed by internal stability functions, in later steps these errors are propagated according to the (principal) stability function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nevertheless, in methods with many stages, internal stability can play a key role.&lt;/p&gt;

&lt;p&gt;Questions: &lt;em&gt;In the solution of PDEs, large spatial truncation errors enter at each stage.  Does this mean internal stability becomes more significant?  How does this relate to stiff accuracy analysis and order reduction?&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;Internal stability functions&lt;/h2&gt;

&lt;p&gt;We can write the equations of a Runge-Kutta method compactly as&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$y = u^n e + h A F(y)$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$u^{n+1} = u^n + h b^T F(y),$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;where $y$ is the vector of stage values, $u^n$ is the previous step solution, $e$ is a vector with all entries equal to 1, $h$ is the step size, $A$ and $b$ are the coefficients in the Butcher tableau, and $F(y)$ is the vector of stage derivatives.  In floating point arithmetic, roundoff errors will be made at each stage.  Representing these errors by a vector $r$, we have&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$y = u^n e + h A F(y) + r.$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Considering the test problem $F(y)=\lambda y$ and solving for $y$ gives&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$y = u^n (I-zA)^{-1}e + (I-zA)^{-1}r,$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;where $z=h\lambda$.  Substituting this result in the equation for $u^{n+1}$ gives&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$u^{n+1} = u^n (1 + zb^T(I-zA)^{-1}e) + zb^T(I-zA)^{-1}r = \psi(z) u^n + \theta(z)^T r.$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Here $\psi(z)$ is the &lt;em&gt;stability function&lt;/em&gt; of the method, that we already encountered above.  Meanwhile, the vector $\theta(z)$ contains the &lt;em&gt;internal stability functions&lt;/em&gt; that govern the amplification of roundoff errors $r$ within a step:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$\theta(z) = z b^T (I-zA)^{-1}.$&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s compute $\theta$ for the classical RK4 method:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_polynomials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    [poly1d([1/24, 1/12, 1/6, 1/6, 0], dtype=object),
     poly1d([1/12, 1/6, 1/3, 0], dtype=object),
     poly1d([1/6, 1/3, 0], dtype=object),
     poly1d([1/6, 0], dtype=object)]
&lt;/pre&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_j&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;         4           3          2
0.04167 x + 0.08333 x + 0.1667 x + 0.1667 x
         3          2
0.08333 x + 0.1667 x + 0.3333 x
        2
0.1667 x + 0.3333 x

0.1667 x
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Thus the roundoff errors in the first stage are amplified by a factor $z^4/24 + z^3/12 + z^2/6 + z/6$, while the errors in the last stage are amplified by a factor $z/6$.&lt;/p&gt;

&lt;h2&gt;Internal instability&lt;/h2&gt;

&lt;p&gt;Usually internal stability is unimportant since it relates to amplification of roundoff errors, which are very small.  Let&amp;#39;s think about when things can go wrong in terms of internal instability.  If $|\theta(z)|$ is of the order $1/\epsilon&lt;em&gt;{machine}$, then roundoff errors could be amplified so much that they destroy the accuracy of the computation.  More specifically, we should be concerned if $|\theta(z)|$ is of the order $tol/\epsilon&lt;/em&gt;{machine}$ where $tol$ is our desired error tolerance.  Of course, we only care about values of $z$ that lie inside the absolute stability region $S$, since internal stability won&amp;#39;t matter if the computation is not absolutely stable.&lt;/p&gt;

&lt;p&gt;We can get some idea about the amplification of stage errors by plotting the curves $|\theta(z)|=1$ along with the stability region.  Ideally these curves will all lie outside the stability region, so that all stage errors are damped.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_02.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_03.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;For both methods, we see that some of the curves intersect the absolute stability region, so some stage errors are amplified.  But by how much?  We&amp;#39;d really like to know the maximum amplification of the stage errors under the condition of absolute stability.  We therefore define the &lt;em&gt;maximum internal amplification factor&lt;/em&gt; $M$:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;$M = \max&lt;em&gt;j \max&lt;/em&gt;{z \in S} |\theta_j(z)|$&lt;/center&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;2.15239281554
4.04399941143
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We see that both methods have small internal amplification factors, so internal stability is not a concern in either case.  This is not surprising for the method with only four stages; it is a surprisingly good property of the method with ten stages.&lt;/p&gt;

&lt;p&gt;Questions: &lt;em&gt;Do SSP RK methods always (necessarily) have small amplification factors?  Can we prove it?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;#39;s look at some methods with many stages.&lt;/p&gt;

&lt;h2&gt;Runge-Kutta Chebyshev methods&lt;/h2&gt;

&lt;p&gt;The paper of Verwer, Hundsdorfer, and Sommeijer deals with RKC methods, which can have very many stages.  The construction of these methods is implemented in NodePy, so let&amp;#39;s take a look at them.  The functions &lt;code&gt;RKC1(s)&lt;/code&gt; and &lt;code&gt;RKC2(s)&lt;/code&gt; construct RKC methods of order 1 and 2, respectively, with $s$ stages.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;RKC41

 0    |
 1/16 |  1/16
 1/4  |  1/8   1/8
 9/16 |  3/16  1/4   1/8
______|________________________
      |   1/4   3/8   1/4   1/8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_04.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;It looks like there could be some significant internal amplification here.  Let&amp;#39;s see:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    11.760869405962685
&lt;/pre&gt;

&lt;p&gt;Nothing catastrophic.  Let&amp;#39;s try a larger value of $s$:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    42.665327220219126
&lt;/pre&gt;

&lt;p&gt;As promised, these methods seem to have good internal stability properties.  What about the second-order methods?&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    106.69110992619214
&lt;/pre&gt;

&lt;p&gt;Again, nothing catastrophic.  We could take $s$ much larger than 20, but the calculations get to be rather slow (in Python) and since we&amp;#39;re using floating point arithmetic, the accuracy deteriorates.&lt;/p&gt;

&lt;p&gt;Remark: &lt;em&gt;we could do the calculations in exact arithmetic using Sympy, but things would get even slower.  Perhaps there are some optimizations that could be done to speed this up.  Or perhaps we should use Mathematica if we need to do this kind of thing.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Remark 2: &lt;em&gt;of course, for the RKC methods the internal stability polynomials are shifted Chebyshev polynomials.  So we could evaluate them directly in a stable manner using the three-term recurrence (or perhaps scipy&amp;#39;s special functions library).  This would also be a nice check on the calculations above.&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;Other methods with many stages&lt;/h2&gt;

&lt;p&gt;Three other classes of methods with many stages have been implemented in NodePy:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SSP families&lt;/li&gt;
&lt;li&gt;Integral deferred correction (IDC) methods&lt;/li&gt;
&lt;li&gt;Extrapolation methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;SSP Families&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSPRK2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    2.0212921484995547
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_05.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# # of stages&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSPRK3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    3.8049237837215397
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_06.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The SSP methods seem to have excellent internal stability properties.&lt;/p&gt;

&lt;h3&gt;IDC methods&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;26
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;
    6.4140166271998815
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_07.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;IDC methods also seem to have excellent internal stability.&lt;/p&gt;

&lt;h3&gt;Extrapolation methods&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;16
6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_08.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Not so good.  Let&amp;#39;s try a method with even more stages (this next computation will take a while; go stretch your legs).&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;46
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;
    28073.244376758907
&lt;/pre&gt;

&lt;p&gt;Now we&amp;#39;re starting to see something that might cause trouble, especially since such high order extrapolation methods are usually used when extremely tight error tolerances are required.  Internal amplification will cause a loss of about 5 digits of accuracy here, so the best we can hope for is about 10 digits of accuracy in double precision.  Higher order extrapolation methods will make things even worse.  How large are their amplification factors?  (Really long calculation here...)&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1 1.99777378912
2 2.40329384375
3
 5.07204078733
4
 17.747335803
5
 69.62805786
6
 97.6097450835
7
 346.277441462
8
 1467.40356089
9
 6344.16303534
10
 28073.2443768
11
 126011.586473
12
 169897.662582
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;
    [&lt;matplotlib.lines.Line2D at 0x2611bbe10&gt;]
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_09.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;semilogy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;
    [&lt;matplotlib.lines.Line2D at 0x2611a6710&gt;]
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_10.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;We see roughly geometric growth of the internal amplification factor as a function of the order $p$.  It seems clear that very high order extrapolation methods applied to problems with high accuracy requirements will fall victim to internal stability issues.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A curious upwind implicit scheme for advection</title>
   <link href="/2012/10/11/A_curious_upwind_implicit_scheme_for_advection.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/A_curious_upwind_implicit_scheme_for_advection</id>
   <content type="html">&lt;h2&gt;The CFL condition&lt;/h2&gt;

&lt;p&gt;The CFL condition is one of the most basic and intuitive principles
in the numerical solution of hyperbolic PDEs. First formulated by
Courant, Friedrichs and Lewy in their seminal paper (in English for
free here](http://www.stat.uchicago.edu/~lekheng/courses/302/classics/courant-friedrichs-lewy.pdf)), it states that the domain of dependence of a numerical
method for solving a PDE must contain the true domain of dependence.
Otherwise, the numerical method cannot be convergent.&lt;/p&gt;

&lt;p&gt;The CFL condition is geometric and easily understood in the context of,
say, a first-order upwind discretization of advection. Usually it says
nothing interesting about implicit schemes, since they include all
points in their domain of dependence. But sometimes understanding the
CFL condition for a particular scheme can be subtle.&lt;/p&gt;

&lt;h3&gt;An implicit scheme&lt;/h3&gt;

&lt;p&gt;Consider the advection equation&lt;/p&gt;

&lt;p&gt;$$u&lt;em&gt;t + a u&lt;/em&gt;x = 0.$$&lt;/p&gt;

&lt;p&gt;Discretization using a backward difference in space and in time gives
the scheme&lt;/p&gt;

&lt;p&gt;$$U^{n+1}&lt;em&gt;j = U^n&lt;/em&gt;j - \nu(U^{n+1}&lt;em&gt;j - U^{n+1}&lt;/em&gt;{j-1}).$$&lt;/p&gt;

&lt;p&gt;Where $\nu = ka/h$ is the CFL number and $k,h$ are the step sizes in
time and space, respectively. This very simple scheme illustrates the
concepts of the CFL condition and stability in a remarkable way.&lt;/p&gt;

&lt;p&gt;For simplicity, suppose that the problem is posed on the domain
$0\le x \le 1$, with an appropriate boundary condition. Since this
scheme computes $U^{n+1}&lt;em&gt;j$ in terms of $U^n&lt;/em&gt;j$ and $U^{n+1}&lt;em&gt;{j-1}$, it
seems that the numerical domain of dependence for $U^n&lt;/em&gt;j$ is
$(x,t)\in (0,x&lt;em&gt;j)\times[0,t&lt;/em&gt;n]$. Based on this, we may conclude that the
scheme is not convergent for $\nu&amp;lt;0$. Simple enough.&lt;/p&gt;

&lt;p&gt;But what if $\nu=-1$? Then the scheme reads $$U^{n+1}&lt;em&gt;{j-1} = U^n&lt;/em&gt;j,$$
which gives &lt;strong&gt;the exact solution&lt;/strong&gt;! This is a sort of “anti-unit CFL
condition”.&lt;/p&gt;

&lt;p&gt;How can this scheme be convergent (in fact, exact!) for a negative CFL
number when it doesn’t use any values to the right?&lt;/p&gt;

&lt;h3&gt;Understanding the CFL condition&lt;/h3&gt;

&lt;p&gt;Look at the exact formula above. In this case the scheme is not a method
for computing $U^{n+1}&lt;em&gt;j$ but for computing $U^{n+1}&lt;/em&gt;{j-1}$, and it
&lt;em&gt;does&lt;/em&gt; use a value from the previous time step that lies to the right.&lt;/p&gt;

&lt;p&gt;So we can view the scheme with $\nu=-1$ as a method for computing
$U^{n+1}&lt;em&gt;j$, in which case the CFL condition is satisfied only for
$\nu\ge0$, or we can view the scheme as a method for computing
$U^{n+1}&lt;/em&gt;{j-1}$, in which case the CFL condition is satisfied only for
$\nu\le-1$. &lt;strong&gt;Which viewpoint is correct?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To answer that question, remember that the CFL condition is purely
algebraic – that is, it relates to which values are actually used to
compute which other values. To understand this scheme, we need to think
about how we actually solve for $U^{n+1}$ when using it. Notice that the
scheme can be written as $$A U^{n+1} = U^n$$ where the matrix $A$ is
lower-triangular. Hence the system can be solved by substitution. To go
further, we must consider two cases:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;$\nu&amp;gt;0$: in this case, boundary values must be supplied along the
left boundary at $x=0$. Then, starting from the known value at the
boundary, we work to the right by substitution:
$$U^{n+1}&lt;em&gt;j = \frac{U^n&lt;/em&gt;j+\nu U^{n+1}&lt;em&gt;{j-1}}{1+\nu}.$$ Hence the
scheme is truly a way of computing $U^{n+1}&lt;/em&gt;j$ based on
$U^n&lt;em&gt;j, U^{n+1}&lt;/em&gt;{j-1}$ and the resulting CFL condition is $\nu\ge0$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\nu&amp;lt;0$: in this case, boundary values must be supplied along the
right boundary at $x=1$. Then, starting from the known value at the
boundary, we work to the left by substitution:
$$U^{n+1}&lt;em&gt;{j-1} = \frac{(1+\nu)U^{n+1}&lt;/em&gt;j - U^n&lt;em&gt;j}{\nu}.$$ Hence the
scheme is truly a way of computing $U^{n+1}&lt;/em&gt;{j-1}$ based on
$U^n&lt;em&gt;j, U^{n+1}&lt;/em&gt;{j}$ and the resulting CFL condition is $\nu\le-1$.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post was originally published on the KAUST Mathwiki &lt;a href=&quot;https://mathwiki.kaust.edu.sa/david/A%20curious%20upwind%20implicit%20scheme%20for%20advection&quot;&gt;here&lt;/a&gt; (login required).&lt;/p&gt;
</content>
 </entry>
 

</feed>
